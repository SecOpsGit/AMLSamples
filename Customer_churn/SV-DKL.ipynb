{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import (CholeskyVariationalDistribution,\n",
    "                                VariationalStrategy)\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "from gpytorch.utils.grid import choose_grid_size\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 199 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67261774 0.85714286 0.         0.99999471 0.         0.59663866\n",
      "  0.         1.         0.96969697 0.74348697 0.03006012 0.07630522\n",
      "  0.82828283 0.35836034 0.46073662 0.         1.         1.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         1.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         1.\n",
      "  0.        ]]\n",
      "(20468, 91)\n",
      "(20468,)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')\n",
    "print(X[0:1,:])\n",
    "print(X.shape, y.shape,sep='\\n')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=.33, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralnetLayer(torch.nn.Sequential):\n",
    "    def __init__(self,data_dim, output_dim):\n",
    "        super(NeuralnetLayer, self).__init__()\n",
    "        #self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        #self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(data_dim, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50,output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessLayer(gpytorch.models.AdditiveGridInducingVariationalGP):\n",
    "    def __init__(self, num_dim, grid_bounds, grid_size):\n",
    "        super(GaussianProcessLayer, self).__init__(grid_size=grid_size, grid_bounds=[grid_bounds],\n",
    "                                                   num_dim=num_dim, mixing_params=False, sum_output=False)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, neuralnet_layer, num_dim, grid_bounds,grid_size):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.neuralnet_layer = neuralnet_layer\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, \n",
    "                                             grid_bounds=grid_bounds,\n",
    "                                            grid_size=grid_size)\n",
    "        self.grid_bounds = grid_bounds\n",
    "        self.num_dim = num_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.neuralnet_layer(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = torch.FloatTensor(X_train), torch.FloatTensor(y_train)\n",
    "\n",
    "dataset = utils.TensorDataset(X_train,y_train)\n",
    "dataloader = utils.DataLoader(dataset, \n",
    "                              batch_size = 128,\n",
    "                              shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "grid_size = 64#choose_grid_size(X_train)\n",
    "print(grid_size)\n",
    "grid_bounds=(-1., 1.)\n",
    "\n",
    "nnet_layer = NeuralnetLayer(data_dim=X_train.size(1),\n",
    "                            output_dim=latent_dim).cuda()\n",
    "\n",
    "model = DKLModel(nnet_layer, num_dim=latent_dim,\n",
    "                           grid_bounds=grid_bounds ,\n",
    "                           grid_size=grid_size).cuda()\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "lr =0.1\n",
    "optimizer = SGD([\n",
    "    {'params': model.neuralnet_layer.parameters()},\n",
    "    {'params': model.gp_layer.hyperparameters(), 'lr': lr * 0.01},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=lr,momentum=0.9, nesterov=True, weight_decay=0)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(dataloader.dataset))\n",
    "\n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -mll(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [020/108], Loss: 1.410385\n",
      "Train Epoch: 1 [040/108], Loss: 1.394321\n",
      "Train Epoch: 1 [060/108], Loss: 1.380714\n",
      "Train Epoch: 1 [080/108], Loss: 1.362059\n",
      "Train Epoch: 1 [100/108], Loss: 1.364481\n",
      "Train Epoch: 2 [020/108], Loss: 1.341274\n",
      "Train Epoch: 2 [040/108], Loss: 1.336330\n",
      "Train Epoch: 2 [060/108], Loss: 1.319339\n",
      "Train Epoch: 2 [080/108], Loss: 1.310478\n",
      "Train Epoch: 2 [100/108], Loss: 1.335119\n",
      "Train Epoch: 3 [020/108], Loss: 1.335832\n",
      "Train Epoch: 3 [040/108], Loss: 1.291161\n",
      "Train Epoch: 3 [060/108], Loss: 1.315436\n",
      "Train Epoch: 3 [080/108], Loss: 1.295143\n",
      "Train Epoch: 3 [100/108], Loss: 1.309992\n",
      "Train Epoch: 4 [020/108], Loss: 1.287485\n",
      "Train Epoch: 4 [040/108], Loss: 1.312078\n",
      "Train Epoch: 4 [060/108], Loss: 1.326498\n",
      "Train Epoch: 4 [080/108], Loss: 1.324845\n",
      "Train Epoch: 4 [100/108], Loss: 1.293575\n",
      "Train Epoch: 5 [020/108], Loss: 1.334522\n",
      "Train Epoch: 5 [040/108], Loss: 1.272888\n",
      "Train Epoch: 5 [060/108], Loss: 1.285204\n",
      "Train Epoch: 5 [080/108], Loss: 1.348242\n",
      "Train Epoch: 5 [100/108], Loss: 1.330834\n",
      "Train Epoch: 6 [020/108], Loss: 1.291393\n",
      "Train Epoch: 6 [040/108], Loss: 1.317891\n",
      "Train Epoch: 6 [060/108], Loss: 1.280792\n",
      "Train Epoch: 6 [080/108], Loss: 1.219984\n",
      "Train Epoch: 6 [100/108], Loss: 1.269829\n",
      "Train Epoch: 7 [020/108], Loss: 1.287134\n",
      "Train Epoch: 7 [040/108], Loss: 1.315749\n",
      "Train Epoch: 7 [060/108], Loss: 1.267125\n",
      "Train Epoch: 7 [080/108], Loss: 1.295495\n",
      "Train Epoch: 7 [100/108], Loss: 1.269295\n",
      "Train Epoch: 8 [020/108], Loss: 1.296047\n",
      "Train Epoch: 8 [040/108], Loss: 1.217297\n",
      "Train Epoch: 8 [060/108], Loss: 1.274819\n",
      "Train Epoch: 8 [080/108], Loss: 1.274209\n",
      "Train Epoch: 8 [100/108], Loss: 1.312365\n",
      "Train Epoch: 9 [020/108], Loss: 1.262939\n",
      "Train Epoch: 9 [040/108], Loss: 1.274473\n",
      "Train Epoch: 9 [060/108], Loss: 1.253095\n",
      "Train Epoch: 9 [080/108], Loss: 1.253029\n",
      "Train Epoch: 9 [100/108], Loss: 1.265876\n",
      "Train Epoch: 10 [020/108], Loss: 1.250992\n",
      "Train Epoch: 10 [040/108], Loss: 1.290775\n",
      "Train Epoch: 10 [060/108], Loss: 1.284179\n",
      "Train Epoch: 10 [080/108], Loss: 1.301362\n",
      "Train Epoch: 10 [100/108], Loss: 1.310109\n",
      "Train Epoch: 11 [020/108], Loss: 1.209799\n",
      "Train Epoch: 11 [040/108], Loss: 1.279386\n",
      "Train Epoch: 11 [060/108], Loss: 1.261026\n",
      "Train Epoch: 11 [080/108], Loss: 1.221192\n",
      "Train Epoch: 11 [100/108], Loss: 1.269029\n",
      "Train Epoch: 12 [020/108], Loss: 1.227695\n",
      "Train Epoch: 12 [040/108], Loss: 1.278877\n",
      "Train Epoch: 12 [060/108], Loss: 1.268585\n",
      "Train Epoch: 12 [080/108], Loss: 1.227665\n",
      "Train Epoch: 12 [100/108], Loss: 1.238113\n",
      "Train Epoch: 13 [020/108], Loss: 1.246961\n",
      "Train Epoch: 13 [040/108], Loss: 1.267736\n",
      "Train Epoch: 13 [060/108], Loss: 1.236599\n",
      "Train Epoch: 13 [080/108], Loss: 1.277018\n",
      "Train Epoch: 13 [100/108], Loss: 1.246689\n",
      "Train Epoch: 14 [020/108], Loss: 1.246523\n",
      "Train Epoch: 14 [040/108], Loss: 1.297346\n",
      "Train Epoch: 14 [060/108], Loss: 1.276560\n",
      "Train Epoch: 14 [080/108], Loss: 1.246198\n",
      "Train Epoch: 14 [100/108], Loss: 1.235197\n",
      "Train Epoch: 15 [020/108], Loss: 1.299110\n",
      "Train Epoch: 15 [040/108], Loss: 1.265697\n",
      "Train Epoch: 15 [060/108], Loss: 1.265674\n",
      "Train Epoch: 15 [080/108], Loss: 1.286620\n",
      "Train Epoch: 15 [100/108], Loss: 1.265700\n",
      "Train Epoch: 16 [020/108], Loss: 1.266937\n",
      "Train Epoch: 16 [040/108], Loss: 1.265473\n",
      "Train Epoch: 16 [060/108], Loss: 1.275611\n",
      "Train Epoch: 16 [080/108], Loss: 1.306434\n",
      "Train Epoch: 16 [100/108], Loss: 1.265125\n",
      "Train Epoch: 17 [020/108], Loss: 1.327243\n",
      "Train Epoch: 17 [040/108], Loss: 1.275556\n",
      "Train Epoch: 17 [060/108], Loss: 1.254368\n",
      "Train Epoch: 17 [080/108], Loss: 1.275286\n",
      "Train Epoch: 17 [100/108], Loss: 1.264740\n",
      "Train Epoch: 18 [020/108], Loss: 1.285462\n",
      "Train Epoch: 18 [040/108], Loss: 1.222864\n",
      "Train Epoch: 18 [060/108], Loss: 1.264270\n",
      "Train Epoch: 18 [080/108], Loss: 1.285147\n",
      "Train Epoch: 18 [100/108], Loss: 1.253528\n",
      "Train Epoch: 19 [020/108], Loss: 1.243589\n",
      "Train Epoch: 19 [040/108], Loss: 1.316983\n",
      "Train Epoch: 19 [060/108], Loss: 1.274772\n",
      "Train Epoch: 19 [080/108], Loss: 1.295170\n",
      "Train Epoch: 19 [100/108], Loss: 1.347053\n",
      "Train Epoch: 20 [020/108], Loss: 1.263576\n",
      "Train Epoch: 20 [040/108], Loss: 1.347781\n",
      "Train Epoch: 20 [060/108], Loss: 1.232406\n",
      "Train Epoch: 20 [080/108], Loss: 1.242278\n",
      "Train Epoch: 20 [100/108], Loss: 1.242598\n",
      "Train Epoch: 21 [020/108], Loss: 1.242633\n",
      "Train Epoch: 21 [040/108], Loss: 1.232003\n",
      "Train Epoch: 21 [060/108], Loss: 1.263200\n",
      "Train Epoch: 21 [080/108], Loss: 1.316237\n",
      "Train Epoch: 21 [100/108], Loss: 1.252687\n",
      "Train Epoch: 22 [020/108], Loss: 1.294642\n",
      "Train Epoch: 22 [040/108], Loss: 1.231635\n",
      "Train Epoch: 22 [060/108], Loss: 1.284049\n",
      "Train Epoch: 22 [080/108], Loss: 1.273482\n",
      "Train Epoch: 22 [100/108], Loss: 1.283829\n",
      "Train Epoch: 23 [020/108], Loss: 1.326419\n",
      "Train Epoch: 23 [040/108], Loss: 1.241301\n",
      "Train Epoch: 23 [060/108], Loss: 1.283542\n",
      "Train Epoch: 23 [080/108], Loss: 1.262450\n",
      "Train Epoch: 23 [100/108], Loss: 1.199432\n",
      "Train Epoch: 24 [020/108], Loss: 1.231098\n",
      "Train Epoch: 24 [040/108], Loss: 1.314886\n",
      "Train Epoch: 24 [060/108], Loss: 1.325367\n",
      "Train Epoch: 24 [080/108], Loss: 1.177931\n",
      "Train Epoch: 24 [100/108], Loss: 1.283333\n",
      "Train Epoch: 25 [020/108], Loss: 1.200539\n",
      "Train Epoch: 25 [040/108], Loss: 1.283650\n",
      "Train Epoch: 25 [060/108], Loss: 1.283097\n",
      "Train Epoch: 25 [080/108], Loss: 1.251762\n",
      "Train Epoch: 25 [100/108], Loss: 1.209496\n",
      "Train Epoch: 26 [020/108], Loss: 1.219832\n",
      "Train Epoch: 26 [040/108], Loss: 1.252104\n",
      "Train Epoch: 26 [060/108], Loss: 1.314588\n",
      "Train Epoch: 26 [080/108], Loss: 1.198907\n",
      "Train Epoch: 26 [100/108], Loss: 1.252240\n",
      "Train Epoch: 27 [020/108], Loss: 1.251032\n",
      "Train Epoch: 27 [040/108], Loss: 1.325216\n",
      "Train Epoch: 27 [060/108], Loss: 1.261683\n",
      "Train Epoch: 27 [080/108], Loss: 1.240255\n",
      "Train Epoch: 27 [100/108], Loss: 1.187546\n",
      "Train Epoch: 28 [020/108], Loss: 1.303701\n",
      "Train Epoch: 28 [040/108], Loss: 1.272457\n",
      "Train Epoch: 28 [060/108], Loss: 1.251171\n",
      "Train Epoch: 28 [080/108], Loss: 1.240613\n",
      "Train Epoch: 28 [100/108], Loss: 1.250727\n",
      "Train Epoch: 29 [020/108], Loss: 1.293320\n",
      "Train Epoch: 29 [040/108], Loss: 1.336150\n",
      "Train Epoch: 29 [060/108], Loss: 1.324982\n",
      "Train Epoch: 29 [080/108], Loss: 1.208425\n",
      "Train Epoch: 29 [100/108], Loss: 1.240220\n",
      "Train Epoch: 30 [020/108], Loss: 1.219027\n",
      "Train Epoch: 30 [040/108], Loss: 1.293348\n",
      "Train Epoch: 30 [060/108], Loss: 1.272111\n",
      "Train Epoch: 30 [080/108], Loss: 1.239981\n",
      "Train Epoch: 30 [100/108], Loss: 1.282232\n",
      "Train Epoch: 31 [020/108], Loss: 1.324902\n",
      "Train Epoch: 31 [040/108], Loss: 1.250644\n",
      "Train Epoch: 31 [060/108], Loss: 1.271607\n",
      "Train Epoch: 31 [080/108], Loss: 1.239461\n",
      "Train Epoch: 31 [100/108], Loss: 1.271636\n",
      "Train Epoch: 32 [020/108], Loss: 1.282398\n",
      "Train Epoch: 32 [040/108], Loss: 1.303596\n",
      "Train Epoch: 32 [060/108], Loss: 1.229368\n",
      "Train Epoch: 32 [080/108], Loss: 1.345787\n",
      "Train Epoch: 32 [100/108], Loss: 1.345672\n",
      "Train Epoch: 33 [020/108], Loss: 1.218543\n",
      "Train Epoch: 33 [040/108], Loss: 1.250427\n",
      "Train Epoch: 33 [060/108], Loss: 1.154506\n",
      "Train Epoch: 33 [080/108], Loss: 1.271405\n",
      "Train Epoch: 33 [100/108], Loss: 1.228945\n",
      "Train Epoch: 34 [020/108], Loss: 1.239307\n",
      "Train Epoch: 34 [040/108], Loss: 1.271259\n",
      "Train Epoch: 34 [060/108], Loss: 1.282532\n",
      "Train Epoch: 34 [080/108], Loss: 1.281943\n",
      "Train Epoch: 34 [100/108], Loss: 1.271448\n",
      "Train Epoch: 35 [020/108], Loss: 1.229132\n",
      "Train Epoch: 35 [040/108], Loss: 1.271398\n",
      "Train Epoch: 35 [060/108], Loss: 1.217915\n",
      "Train Epoch: 35 [080/108], Loss: 1.292373\n",
      "Train Epoch: 35 [100/108], Loss: 1.249687\n",
      "Train Epoch: 36 [020/108], Loss: 1.302974\n",
      "Train Epoch: 36 [040/108], Loss: 1.303161\n",
      "Train Epoch: 36 [060/108], Loss: 1.281760\n",
      "Train Epoch: 36 [080/108], Loss: 1.239401\n",
      "Train Epoch: 36 [100/108], Loss: 1.281817\n",
      "Train Epoch: 37 [020/108], Loss: 1.345279\n",
      "Train Epoch: 37 [040/108], Loss: 1.185942\n",
      "Train Epoch: 37 [060/108], Loss: 1.292460\n",
      "Train Epoch: 37 [080/108], Loss: 1.260691\n",
      "Train Epoch: 37 [100/108], Loss: 1.292431\n",
      "Train Epoch: 38 [020/108], Loss: 1.260973\n",
      "Train Epoch: 38 [040/108], Loss: 1.270949\n",
      "Train Epoch: 38 [060/108], Loss: 1.249933\n",
      "Train Epoch: 38 [080/108], Loss: 1.271089\n",
      "Train Epoch: 38 [100/108], Loss: 1.196078\n",
      "Train Epoch: 39 [020/108], Loss: 1.239120\n",
      "Train Epoch: 39 [040/108], Loss: 1.292283\n",
      "Train Epoch: 39 [060/108], Loss: 1.313478\n",
      "Train Epoch: 39 [080/108], Loss: 1.207175\n",
      "Train Epoch: 39 [100/108], Loss: 1.292054\n",
      "Train Epoch: 40 [020/108], Loss: 1.249770\n",
      "Train Epoch: 40 [040/108], Loss: 1.281807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 40 [060/108], Loss: 1.228277\n",
      "Train Epoch: 40 [080/108], Loss: 1.259711\n",
      "Train Epoch: 40 [100/108], Loss: 1.217582\n",
      "Train Epoch: 41 [020/108], Loss: 1.239269\n",
      "Train Epoch: 41 [040/108], Loss: 1.260476\n",
      "Train Epoch: 41 [060/108], Loss: 1.249617\n",
      "Train Epoch: 41 [080/108], Loss: 1.249351\n",
      "Train Epoch: 41 [100/108], Loss: 1.292289\n",
      "Train Epoch: 42 [020/108], Loss: 1.260226\n",
      "Train Epoch: 42 [040/108], Loss: 1.334671\n",
      "Train Epoch: 42 [060/108], Loss: 1.302510\n",
      "Train Epoch: 42 [080/108], Loss: 1.302796\n",
      "Train Epoch: 42 [100/108], Loss: 1.292064\n",
      "Train Epoch: 43 [020/108], Loss: 1.249354\n",
      "Train Epoch: 43 [040/108], Loss: 1.270616\n",
      "Train Epoch: 43 [060/108], Loss: 1.249219\n",
      "Train Epoch: 43 [080/108], Loss: 1.195734\n",
      "Train Epoch: 43 [100/108], Loss: 1.259824\n",
      "Train Epoch: 44 [020/108], Loss: 1.281700\n",
      "Train Epoch: 44 [040/108], Loss: 1.292273\n",
      "Train Epoch: 44 [060/108], Loss: 1.238421\n",
      "Train Epoch: 44 [080/108], Loss: 1.238400\n",
      "Train Epoch: 44 [100/108], Loss: 1.217451\n",
      "Train Epoch: 45 [020/108], Loss: 1.238592\n",
      "Train Epoch: 45 [040/108], Loss: 1.270295\n",
      "Train Epoch: 45 [060/108], Loss: 1.259591\n",
      "Train Epoch: 45 [080/108], Loss: 1.270605\n",
      "Train Epoch: 45 [100/108], Loss: 1.238746\n",
      "Train Epoch: 46 [020/108], Loss: 1.270295\n",
      "Train Epoch: 46 [040/108], Loss: 1.239125\n",
      "Train Epoch: 46 [060/108], Loss: 1.291688\n",
      "Train Epoch: 46 [080/108], Loss: 1.291703\n",
      "Train Epoch: 46 [100/108], Loss: 1.323614\n",
      "Train Epoch: 47 [020/108], Loss: 1.238167\n",
      "Train Epoch: 47 [040/108], Loss: 1.227315\n",
      "Train Epoch: 47 [060/108], Loss: 1.313202\n",
      "Train Epoch: 47 [080/108], Loss: 1.227996\n",
      "Train Epoch: 47 [100/108], Loss: 1.259704\n",
      "Train Epoch: 48 [020/108], Loss: 1.302747\n",
      "Train Epoch: 48 [040/108], Loss: 1.302794\n",
      "Train Epoch: 48 [060/108], Loss: 1.259553\n",
      "Train Epoch: 48 [080/108], Loss: 1.324325\n",
      "Train Epoch: 48 [100/108], Loss: 1.259839\n",
      "Train Epoch: 49 [020/108], Loss: 1.269916\n",
      "Train Epoch: 49 [040/108], Loss: 1.259514\n",
      "Train Epoch: 49 [060/108], Loss: 1.291761\n",
      "Train Epoch: 49 [080/108], Loss: 1.312934\n",
      "Train Epoch: 49 [100/108], Loss: 1.291880\n",
      "Train Epoch: 50 [020/108], Loss: 1.280766\n",
      "Train Epoch: 50 [040/108], Loss: 1.302454\n",
      "Train Epoch: 50 [060/108], Loss: 1.248983\n",
      "Train Epoch: 50 [080/108], Loss: 1.238077\n",
      "Train Epoch: 50 [100/108], Loss: 1.281158\n",
      "Train Epoch: 51 [020/108], Loss: 1.248975\n",
      "Train Epoch: 51 [040/108], Loss: 1.237943\n",
      "Train Epoch: 51 [060/108], Loss: 1.259794\n",
      "Train Epoch: 51 [080/108], Loss: 1.291608\n",
      "Train Epoch: 51 [100/108], Loss: 1.291286\n",
      "Train Epoch: 52 [020/108], Loss: 1.259364\n",
      "Train Epoch: 52 [040/108], Loss: 1.227543\n",
      "Train Epoch: 52 [060/108], Loss: 1.216528\n",
      "Train Epoch: 52 [080/108], Loss: 1.270267\n",
      "Train Epoch: 52 [100/108], Loss: 1.270439\n",
      "Train Epoch: 53 [020/108], Loss: 1.249087\n",
      "Train Epoch: 53 [040/108], Loss: 1.153493\n",
      "Train Epoch: 53 [060/108], Loss: 1.291686\n",
      "Train Epoch: 53 [080/108], Loss: 1.280939\n",
      "Train Epoch: 53 [100/108], Loss: 1.291474\n",
      "Train Epoch: 54 [020/108], Loss: 1.281026\n",
      "Train Epoch: 54 [040/108], Loss: 1.281093\n",
      "Train Epoch: 54 [060/108], Loss: 1.259463\n",
      "Train Epoch: 54 [080/108], Loss: 1.291704\n",
      "Train Epoch: 54 [100/108], Loss: 1.238128\n",
      "Train Epoch: 55 [020/108], Loss: 1.237942\n",
      "Train Epoch: 55 [040/108], Loss: 1.345412\n",
      "Train Epoch: 55 [060/108], Loss: 1.270292\n",
      "Train Epoch: 55 [080/108], Loss: 1.249076\n",
      "Train Epoch: 55 [100/108], Loss: 1.291578\n",
      "Train Epoch: 56 [020/108], Loss: 1.227246\n",
      "Train Epoch: 56 [040/108], Loss: 1.280718\n",
      "Train Epoch: 56 [060/108], Loss: 1.270297\n",
      "Train Epoch: 56 [080/108], Loss: 1.259355\n",
      "Train Epoch: 56 [100/108], Loss: 1.248925\n",
      "Train Epoch: 57 [020/108], Loss: 1.227502\n",
      "Train Epoch: 57 [040/108], Loss: 1.323438\n",
      "Train Epoch: 57 [060/108], Loss: 1.270602\n",
      "Train Epoch: 57 [080/108], Loss: 1.270040\n",
      "Train Epoch: 57 [100/108], Loss: 1.270464\n",
      "Train Epoch: 58 [020/108], Loss: 1.291990\n",
      "Train Epoch: 58 [040/108], Loss: 1.259529\n",
      "Train Epoch: 58 [060/108], Loss: 1.227298\n",
      "Train Epoch: 58 [080/108], Loss: 1.249061\n",
      "Train Epoch: 58 [100/108], Loss: 1.206121\n",
      "Train Epoch: 59 [020/108], Loss: 1.259436\n",
      "Train Epoch: 59 [040/108], Loss: 1.312866\n",
      "Train Epoch: 59 [060/108], Loss: 1.248725\n",
      "Train Epoch: 59 [080/108], Loss: 1.345263\n",
      "Train Epoch: 59 [100/108], Loss: 1.291619\n",
      "Train Epoch: 60 [020/108], Loss: 1.259419\n",
      "Train Epoch: 60 [040/108], Loss: 1.302971\n",
      "Train Epoch: 60 [060/108], Loss: 1.227214\n",
      "Train Epoch: 60 [080/108], Loss: 1.237907\n",
      "Train Epoch: 60 [100/108], Loss: 1.302245\n",
      "Train Epoch: 61 [020/108], Loss: 1.291360\n",
      "Train Epoch: 61 [040/108], Loss: 1.259252\n",
      "Train Epoch: 61 [060/108], Loss: 1.259556\n",
      "Train Epoch: 61 [080/108], Loss: 1.312925\n",
      "Train Epoch: 61 [100/108], Loss: 1.206025\n",
      "Train Epoch: 62 [020/108], Loss: 1.280842\n",
      "Train Epoch: 62 [040/108], Loss: 1.302156\n",
      "Train Epoch: 62 [060/108], Loss: 1.270298\n",
      "Train Epoch: 62 [080/108], Loss: 1.270538\n",
      "Train Epoch: 62 [100/108], Loss: 1.248542\n",
      "Train Epoch: 63 [020/108], Loss: 1.259489\n",
      "Train Epoch: 63 [040/108], Loss: 1.248818\n",
      "Train Epoch: 63 [060/108], Loss: 1.248870\n",
      "Train Epoch: 63 [080/108], Loss: 1.206130\n",
      "Train Epoch: 63 [100/108], Loss: 1.216692\n",
      "Train Epoch: 64 [020/108], Loss: 1.291569\n",
      "Train Epoch: 64 [040/108], Loss: 1.260210\n",
      "Train Epoch: 64 [060/108], Loss: 1.313110\n",
      "Train Epoch: 64 [080/108], Loss: 1.280811\n",
      "Train Epoch: 64 [100/108], Loss: 1.270399\n",
      "Train Epoch: 65 [020/108], Loss: 1.280812\n",
      "Train Epoch: 65 [040/108], Loss: 1.291675\n",
      "Train Epoch: 65 [060/108], Loss: 1.280935\n",
      "Train Epoch: 65 [080/108], Loss: 1.302267\n",
      "Train Epoch: 65 [100/108], Loss: 1.312778\n",
      "Train Epoch: 66 [020/108], Loss: 1.280753\n",
      "Train Epoch: 66 [040/108], Loss: 1.238070\n",
      "Train Epoch: 66 [060/108], Loss: 1.345125\n",
      "Train Epoch: 66 [080/108], Loss: 1.281088\n",
      "Train Epoch: 66 [100/108], Loss: 1.302287\n",
      "Train Epoch: 67 [020/108], Loss: 1.313328\n",
      "Train Epoch: 67 [040/108], Loss: 1.280847\n",
      "Train Epoch: 67 [060/108], Loss: 1.205680\n",
      "Train Epoch: 67 [080/108], Loss: 1.238108\n",
      "Train Epoch: 67 [100/108], Loss: 1.313019\n",
      "Train Epoch: 68 [020/108], Loss: 1.323647\n",
      "Train Epoch: 68 [040/108], Loss: 1.313193\n",
      "Train Epoch: 68 [060/108], Loss: 1.248972\n",
      "Train Epoch: 68 [080/108], Loss: 1.269924\n",
      "Train Epoch: 68 [100/108], Loss: 1.280893\n",
      "Train Epoch: 69 [020/108], Loss: 1.227230\n",
      "Train Epoch: 69 [040/108], Loss: 1.270504\n",
      "Train Epoch: 69 [060/108], Loss: 1.312959\n",
      "Train Epoch: 69 [080/108], Loss: 1.280993\n",
      "Train Epoch: 69 [100/108], Loss: 1.312918\n",
      "Train Epoch: 70 [020/108], Loss: 1.291425\n",
      "Train Epoch: 70 [040/108], Loss: 1.334396\n",
      "Train Epoch: 70 [060/108], Loss: 1.323777\n",
      "Train Epoch: 70 [080/108], Loss: 1.323807\n",
      "Train Epoch: 70 [100/108], Loss: 1.259709\n",
      "Train Epoch: 71 [020/108], Loss: 1.248569\n",
      "Train Epoch: 71 [040/108], Loss: 1.206265\n",
      "Train Epoch: 71 [060/108], Loss: 1.291679\n",
      "Train Epoch: 71 [080/108], Loss: 1.173759\n",
      "Train Epoch: 71 [100/108], Loss: 1.312729\n",
      "Train Epoch: 72 [020/108], Loss: 1.259170\n",
      "Train Epoch: 72 [040/108], Loss: 1.227180\n",
      "Train Epoch: 72 [060/108], Loss: 1.205977\n",
      "Train Epoch: 72 [080/108], Loss: 1.248661\n",
      "Train Epoch: 72 [100/108], Loss: 1.313475\n",
      "Train Epoch: 73 [020/108], Loss: 1.259213\n",
      "Train Epoch: 73 [040/108], Loss: 1.312774\n",
      "Train Epoch: 73 [060/108], Loss: 1.227545\n",
      "Train Epoch: 73 [080/108], Loss: 1.302154\n",
      "Train Epoch: 73 [100/108], Loss: 1.291727\n",
      "Train Epoch: 74 [020/108], Loss: 1.237770\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    scheduler.step()\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amlenv]",
   "language": "python",
   "name": "conda-env-amlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
