{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import (CholeskyVariationalDistribution,\n",
    "                                VariationalStrategy,\n",
    "                                 WhitenedVariationalStrategy,\n",
    "                                 AdditiveGridInterpolationVariationalStrategy)\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "from gpytorch.utils.grid import choose_grid_size\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 199 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67261774 0.85714286 0.         0.99999471 0.         0.59663866\n",
      "  0.         1.         0.96969697 0.74348697 0.03006012 0.07630522\n",
      "  0.82828283 0.35836034 0.46073662 0.         1.         1.\n",
      "  0.         0.         0.         0.         1.         0.\n",
      "  1.         0.         1.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         1.         0.         1.\n",
      "  0.         0.         1.         0.         0.         1.\n",
      "  0.        ]]\n",
      "(20468, 91)\n",
      "(20468,)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')\n",
    "print(X[0:1,:])\n",
    "print(X.shape, y.shape,sep='\\n')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=.33, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralnetLayer(torch.nn.Sequential):\n",
    "    def __init__(self,data_dim, output_dim):\n",
    "        super(NeuralnetLayer, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('bn1', torch.nn.BatchNorm1d(1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())                    \n",
    "        self.add_module('linear3', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('bn3', torch.nn.BatchNorm1d(500))\n",
    "        self.add_module('relu3', torch.nn.ReLU())                  \n",
    "        self.add_module('linear4', torch.nn.Linear(500, 50))       \n",
    "        self.add_module('bn4', torch.nn.BatchNorm1d(50))\n",
    "        self.add_module('relu4', torch.nn.ReLU())   \n",
    "        self.add_module('linear5', torch.nn.Linear(50,output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessLayer(AbstractVariationalGP):\n",
    "    def __init__(self, num_dim, grid_bounds, grid_size):\n",
    "        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=grid_size,\n",
    "                                                                  batch_size=num_dim)\n",
    "       \n",
    "        variational_strategy = AdditiveGridInterpolationVariationalStrategy(self,\n",
    "                                                                            grid_size=grid_size,\n",
    "                                                                            grid_bounds=[grid_bounds],\n",
    "                                                                            num_dim=num_dim,\n",
    "                                                                            variational_distribution=variational_distribution)\n",
    "        super(GaussianProcessLayer,self).__init__(variational_strategy)\n",
    "\n",
    "        '''super(GaussianProcessLayer, self).__init__(grid_size=grid_size, grid_bounds=grid_bounds,\n",
    "                                                   num_dim=num_dim, mixing_params=False, sum_output=False)'''\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
    "                    math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLModel(gpytorch.Module):\n",
    "    def __init__(self, nnet_layer, num_dim, \n",
    "                 grid_bounds,grid_size):\n",
    "        super(DKLModel, self).__init__()\n",
    "        self.nnet_layer = nnet_layer\n",
    "        self.gp_layer = GaussianProcessLayer(num_dim=num_dim, \n",
    "                                             grid_bounds=grid_bounds,\n",
    "                                             grid_size=grid_size)\n",
    "        self.grid_bounds = grid_bounds\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.nnet_layer(x)\n",
    "        features = gpytorch.utils.grid.scale_to_bounds(features, self.grid_bounds[0], self.grid_bounds[1])\n",
    "        res = self.gp_layer(features)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = torch.FloatTensor(X_train), torch.FloatTensor(y_train)\n",
    "batch_size = 1024\n",
    "dataset = utils.TensorDataset(X_train,y_train)\n",
    "dataloader = utils.DataLoader(dataset, \n",
    "                              batch_size = batch_size,\n",
    "                              shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "grid_size = 64 #choose_grid_size(X_train)\n",
    "print(grid_size)\n",
    "grid_bounds=(-1,1)\n",
    "\n",
    "nnet_layer = NeuralnetLayer(data_dim=X_train.size(1),\n",
    "                            output_dim=latent_dim).cuda()\n",
    "\n",
    "model = DKLModel(nnet_layer,num_dim=latent_dim,\n",
    "                           grid_bounds=grid_bounds,\n",
    "                           grid_size=grid_size).cuda()\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "lr =0.1\n",
    "optimizer = SGD([\n",
    "    {'params': model.nnet_layer.parameters(),'lr':1e-4, 'weight_decay': 1e-3},\n",
    "    {'params': model.gp_layer.hyperparameters(), 'lr': lr * 0.01},\n",
    "    {'params': model.gp_layer.variational_parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=lr)#,momentum=0.9, nesterov=True, weight_decay=0)\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[0.5 * n_epochs, 0.75 * n_epochs], gamma=0.1)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=y_train.numel())\n",
    "\n",
    "    train_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = -mll(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx + 1) % 2 == 0:\n",
    "            print('Train Epoch: %d [%03d/%03d], Loss: %.6f' % (epoch, batch_idx + 1, len(dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [002/014], Loss: 0.634688\n",
      "Train Epoch: 1 [004/014], Loss: 0.629350\n",
      "Train Epoch: 1 [006/014], Loss: 0.635888\n",
      "Train Epoch: 1 [008/014], Loss: 0.644027\n",
      "Train Epoch: 1 [010/014], Loss: 0.617084\n",
      "Train Epoch: 1 [012/014], Loss: 0.634751\n",
      "Train Epoch: 1 [014/014], Loss: 0.629391\n",
      "Train Epoch: 2 [002/014], Loss: 0.636655\n",
      "Train Epoch: 2 [004/014], Loss: 0.628432\n",
      "Train Epoch: 2 [006/014], Loss: 0.633204\n",
      "Train Epoch: 2 [008/014], Loss: 0.627791\n",
      "Train Epoch: 2 [010/014], Loss: 0.640163\n",
      "Train Epoch: 2 [012/014], Loss: 0.629969\n",
      "Train Epoch: 2 [014/014], Loss: 0.626084\n",
      "Train Epoch: 3 [002/014], Loss: 0.633129\n",
      "Train Epoch: 3 [004/014], Loss: 0.629900\n",
      "Train Epoch: 3 [006/014], Loss: 0.635258\n",
      "Train Epoch: 3 [008/014], Loss: 0.623762\n",
      "Train Epoch: 3 [010/014], Loss: 0.628447\n",
      "Train Epoch: 3 [012/014], Loss: 0.640689\n",
      "Train Epoch: 3 [014/014], Loss: 0.619145\n",
      "Train Epoch: 4 [002/014], Loss: 0.628312\n",
      "Train Epoch: 4 [004/014], Loss: 0.634480\n",
      "Train Epoch: 4 [006/014], Loss: 0.643353\n",
      "Train Epoch: 4 [008/014], Loss: 0.627196\n",
      "Train Epoch: 4 [010/014], Loss: 0.632805\n",
      "Train Epoch: 4 [012/014], Loss: 0.632722\n",
      "Train Epoch: 4 [014/014], Loss: 0.624328\n",
      "Train Epoch: 5 [002/014], Loss: 0.635189\n",
      "Train Epoch: 5 [004/014], Loss: 0.641377\n",
      "Train Epoch: 5 [006/014], Loss: 0.631153\n",
      "Train Epoch: 5 [008/014], Loss: 0.627997\n",
      "Train Epoch: 5 [010/014], Loss: 0.628543\n",
      "Train Epoch: 5 [012/014], Loss: 0.625134\n",
      "Train Epoch: 5 [014/014], Loss: 0.636514\n",
      "Train Epoch: 6 [002/014], Loss: 0.635932\n",
      "Train Epoch: 6 [004/014], Loss: 0.628406\n",
      "Train Epoch: 6 [006/014], Loss: 0.640527\n",
      "Train Epoch: 6 [008/014], Loss: 0.633239\n",
      "Train Epoch: 6 [010/014], Loss: 0.630502\n",
      "Train Epoch: 6 [012/014], Loss: 0.626676\n",
      "Train Epoch: 6 [014/014], Loss: 0.627811\n",
      "Train Epoch: 7 [002/014], Loss: 0.638946\n",
      "Train Epoch: 7 [004/014], Loss: 0.621663\n",
      "Train Epoch: 7 [006/014], Loss: 0.623609\n",
      "Train Epoch: 7 [008/014], Loss: 0.627019\n",
      "Train Epoch: 7 [010/014], Loss: 0.633874\n",
      "Train Epoch: 7 [012/014], Loss: 0.626060\n",
      "Train Epoch: 7 [014/014], Loss: 0.646921\n",
      "Train Epoch: 8 [002/014], Loss: 0.635655\n",
      "Train Epoch: 8 [004/014], Loss: 0.624401\n",
      "Train Epoch: 8 [006/014], Loss: 0.642122\n",
      "Train Epoch: 8 [008/014], Loss: 0.619087\n",
      "Train Epoch: 8 [010/014], Loss: 0.634596\n",
      "Train Epoch: 8 [012/014], Loss: 0.633435\n",
      "Train Epoch: 8 [014/014], Loss: 0.641631\n",
      "Train Epoch: 9 [002/014], Loss: 0.637833\n",
      "Train Epoch: 9 [004/014], Loss: 0.633815\n",
      "Train Epoch: 9 [006/014], Loss: 0.638607\n",
      "Train Epoch: 9 [008/014], Loss: 0.626397\n",
      "Train Epoch: 9 [010/014], Loss: 0.629407\n",
      "Train Epoch: 9 [012/014], Loss: 0.630418\n",
      "Train Epoch: 9 [014/014], Loss: 0.629608\n",
      "Train Epoch: 10 [002/014], Loss: 0.625237\n",
      "Train Epoch: 10 [004/014], Loss: 0.618446\n",
      "Train Epoch: 10 [006/014], Loss: 0.631260\n",
      "Train Epoch: 10 [008/014], Loss: 0.627356\n",
      "Train Epoch: 10 [010/014], Loss: 0.637497\n",
      "Train Epoch: 10 [012/014], Loss: 0.635362\n",
      "Train Epoch: 10 [014/014], Loss: 0.636588\n",
      "Train Epoch: 11 [002/014], Loss: 0.630429\n",
      "Train Epoch: 11 [004/014], Loss: 0.630425\n",
      "Train Epoch: 11 [006/014], Loss: 0.635490\n",
      "Train Epoch: 11 [008/014], Loss: 0.646278\n",
      "Train Epoch: 11 [010/014], Loss: 0.623187\n",
      "Train Epoch: 11 [012/014], Loss: 0.634659\n",
      "Train Epoch: 11 [014/014], Loss: 0.624279\n",
      "Train Epoch: 12 [002/014], Loss: 0.624304\n",
      "Train Epoch: 12 [004/014], Loss: 0.631919\n",
      "Train Epoch: 12 [006/014], Loss: 0.635271\n",
      "Train Epoch: 12 [008/014], Loss: 0.633181\n",
      "Train Epoch: 12 [010/014], Loss: 0.633191\n",
      "Train Epoch: 12 [012/014], Loss: 0.636559\n",
      "Train Epoch: 12 [014/014], Loss: 0.619171\n",
      "Train Epoch: 13 [002/014], Loss: 0.626281\n",
      "Train Epoch: 13 [004/014], Loss: 0.632337\n",
      "Train Epoch: 13 [006/014], Loss: 0.621218\n",
      "Train Epoch: 13 [008/014], Loss: 0.631142\n",
      "Train Epoch: 13 [010/014], Loss: 0.630607\n",
      "Train Epoch: 13 [012/014], Loss: 0.633838\n",
      "Train Epoch: 13 [014/014], Loss: 0.633208\n",
      "Train Epoch: 14 [002/014], Loss: 0.638109\n",
      "Train Epoch: 14 [004/014], Loss: 0.629896\n",
      "Train Epoch: 14 [006/014], Loss: 0.630763\n",
      "Train Epoch: 14 [008/014], Loss: 0.622505\n",
      "Train Epoch: 14 [010/014], Loss: 0.633880\n",
      "Train Epoch: 14 [012/014], Loss: 0.638171\n",
      "Train Epoch: 14 [014/014], Loss: 0.629507\n",
      "Train Epoch: 15 [002/014], Loss: 0.631759\n",
      "Train Epoch: 15 [004/014], Loss: 0.632624\n",
      "Train Epoch: 15 [006/014], Loss: 0.651451\n",
      "Train Epoch: 15 [008/014], Loss: 0.625729\n",
      "Train Epoch: 15 [010/014], Loss: 0.625121\n",
      "Train Epoch: 15 [012/014], Loss: 0.633182\n",
      "Train Epoch: 15 [014/014], Loss: 0.613991\n",
      "Train Epoch: 16 [002/014], Loss: 0.626120\n",
      "Train Epoch: 16 [004/014], Loss: 0.631859\n",
      "Train Epoch: 16 [006/014], Loss: 0.627240\n",
      "Train Epoch: 16 [008/014], Loss: 0.640075\n",
      "Train Epoch: 16 [010/014], Loss: 0.631324\n",
      "Train Epoch: 16 [012/014], Loss: 0.640276\n",
      "Train Epoch: 16 [014/014], Loss: 0.634738\n",
      "Train Epoch: 17 [002/014], Loss: 0.633902\n",
      "Train Epoch: 17 [004/014], Loss: 0.631183\n",
      "Train Epoch: 17 [006/014], Loss: 0.632022\n",
      "Train Epoch: 17 [008/014], Loss: 0.629816\n",
      "Train Epoch: 17 [010/014], Loss: 0.627038\n",
      "Train Epoch: 17 [012/014], Loss: 0.627660\n",
      "Train Epoch: 17 [014/014], Loss: 0.643439\n",
      "Train Epoch: 18 [002/014], Loss: 0.625914\n",
      "Train Epoch: 18 [004/014], Loss: 0.636122\n",
      "Train Epoch: 18 [006/014], Loss: 0.631878\n",
      "Train Epoch: 18 [008/014], Loss: 0.631138\n",
      "Train Epoch: 18 [010/014], Loss: 0.640697\n",
      "Train Epoch: 18 [012/014], Loss: 0.632787\n",
      "Train Epoch: 18 [014/014], Loss: 0.638210\n",
      "Train Epoch: 19 [002/014], Loss: 0.637749\n",
      "Train Epoch: 19 [004/014], Loss: 0.637493\n",
      "Train Epoch: 19 [006/014], Loss: 0.633239\n",
      "Train Epoch: 19 [008/014], Loss: 0.633542\n",
      "Train Epoch: 19 [010/014], Loss: 0.633852\n",
      "Train Epoch: 19 [012/014], Loss: 0.632480\n",
      "Train Epoch: 19 [014/014], Loss: 0.638123\n",
      "Train Epoch: 20 [002/014], Loss: 0.635410\n",
      "Train Epoch: 20 [004/014], Loss: 0.636001\n",
      "Train Epoch: 20 [006/014], Loss: 0.635305\n",
      "Train Epoch: 20 [008/014], Loss: 0.623688\n",
      "Train Epoch: 20 [010/014], Loss: 0.631411\n",
      "Train Epoch: 20 [012/014], Loss: 0.629178\n",
      "Train Epoch: 20 [014/014], Loss: 0.643520\n",
      "Train Epoch: 21 [002/014], Loss: 0.618960\n",
      "Train Epoch: 21 [004/014], Loss: 0.631380\n",
      "Train Epoch: 21 [006/014], Loss: 0.629452\n",
      "Train Epoch: 21 [008/014], Loss: 0.639361\n",
      "Train Epoch: 21 [010/014], Loss: 0.628441\n",
      "Train Epoch: 21 [012/014], Loss: 0.637837\n",
      "Train Epoch: 21 [014/014], Loss: 0.635020\n",
      "Train Epoch: 22 [002/014], Loss: 0.633887\n",
      "Train Epoch: 22 [004/014], Loss: 0.623950\n",
      "Train Epoch: 22 [006/014], Loss: 0.642658\n",
      "Train Epoch: 22 [008/014], Loss: 0.629197\n",
      "Train Epoch: 22 [010/014], Loss: 0.629297\n",
      "Train Epoch: 22 [012/014], Loss: 0.623803\n",
      "Train Epoch: 22 [014/014], Loss: 0.629508\n",
      "Train Epoch: 23 [002/014], Loss: 0.633917\n",
      "Train Epoch: 23 [004/014], Loss: 0.635308\n",
      "Train Epoch: 23 [006/014], Loss: 0.640191\n",
      "Train Epoch: 23 [008/014], Loss: 0.618919\n",
      "Train Epoch: 23 [010/014], Loss: 0.627728\n",
      "Train Epoch: 23 [012/014], Loss: 0.623794\n",
      "Train Epoch: 23 [014/014], Loss: 0.645420\n",
      "Train Epoch: 24 [002/014], Loss: 0.627878\n",
      "Train Epoch: 24 [004/014], Loss: 0.640686\n",
      "Train Epoch: 24 [006/014], Loss: 0.641395\n",
      "Train Epoch: 24 [008/014], Loss: 0.635284\n",
      "Train Epoch: 24 [010/014], Loss: 0.626371\n",
      "Train Epoch: 24 [012/014], Loss: 0.627832\n",
      "Train Epoch: 24 [014/014], Loss: 0.632937\n",
      "Train Epoch: 25 [002/014], Loss: 0.632429\n",
      "Train Epoch: 25 [004/014], Loss: 0.633121\n",
      "Train Epoch: 25 [006/014], Loss: 0.636438\n",
      "Train Epoch: 25 [008/014], Loss: 0.622519\n",
      "Train Epoch: 25 [010/014], Loss: 0.628531\n",
      "Train Epoch: 25 [012/014], Loss: 0.627809\n",
      "Train Epoch: 25 [014/014], Loss: 0.633057\n",
      "Train Epoch: 26 [002/014], Loss: 0.633248\n",
      "Train Epoch: 26 [004/014], Loss: 0.631321\n",
      "Train Epoch: 26 [006/014], Loss: 0.640676\n",
      "Train Epoch: 26 [008/014], Loss: 0.637440\n",
      "Train Epoch: 26 [010/014], Loss: 0.626341\n",
      "Train Epoch: 26 [012/014], Loss: 0.635322\n",
      "Train Epoch: 26 [014/014], Loss: 0.645150\n",
      "Train Epoch: 27 [002/014], Loss: 0.622449\n",
      "Train Epoch: 27 [004/014], Loss: 0.616861\n",
      "Train Epoch: 27 [006/014], Loss: 0.625968\n",
      "Train Epoch: 27 [008/014], Loss: 0.627910\n",
      "Train Epoch: 27 [010/014], Loss: 0.631949\n",
      "Train Epoch: 27 [012/014], Loss: 0.624988\n",
      "Train Epoch: 27 [014/014], Loss: 0.634617\n",
      "Train Epoch: 28 [002/014], Loss: 0.621071\n",
      "Train Epoch: 28 [004/014], Loss: 0.625007\n",
      "Train Epoch: 28 [006/014], Loss: 0.631328\n",
      "Train Epoch: 28 [008/014], Loss: 0.636679\n",
      "Train Epoch: 28 [010/014], Loss: 0.635152\n",
      "Train Epoch: 28 [012/014], Loss: 0.630748\n",
      "Train Epoch: 28 [014/014], Loss: 0.641577\n",
      "Train Epoch: 29 [002/014], Loss: 0.631371\n",
      "Train Epoch: 29 [004/014], Loss: 0.627633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [006/014], Loss: 0.631776\n",
      "Train Epoch: 29 [008/014], Loss: 0.644962\n",
      "Train Epoch: 29 [010/014], Loss: 0.636654\n",
      "Train Epoch: 29 [012/014], Loss: 0.627081\n",
      "Train Epoch: 29 [014/014], Loss: 0.627756\n",
      "Train Epoch: 30 [002/014], Loss: 0.633075\n",
      "Train Epoch: 30 [004/014], Loss: 0.627094\n",
      "Train Epoch: 30 [006/014], Loss: 0.634445\n",
      "Train Epoch: 30 [008/014], Loss: 0.625195\n",
      "Train Epoch: 30 [010/014], Loss: 0.633738\n",
      "Train Epoch: 30 [012/014], Loss: 0.634777\n",
      "Train Epoch: 30 [014/014], Loss: 0.638148\n",
      "Train Epoch: 31 [002/014], Loss: 0.638690\n",
      "Train Epoch: 31 [004/014], Loss: 0.635842\n",
      "Train Epoch: 31 [006/014], Loss: 0.633265\n",
      "Train Epoch: 31 [008/014], Loss: 0.622351\n",
      "Train Epoch: 31 [010/014], Loss: 0.621696\n",
      "Train Epoch: 31 [012/014], Loss: 0.635353\n",
      "Train Epoch: 31 [014/014], Loss: 0.625988\n",
      "Train Epoch: 32 [002/014], Loss: 0.638653\n",
      "Train Epoch: 32 [004/014], Loss: 0.619006\n",
      "Train Epoch: 32 [006/014], Loss: 0.624441\n",
      "Train Epoch: 32 [008/014], Loss: 0.638604\n",
      "Train Epoch: 32 [010/014], Loss: 0.634465\n",
      "Train Epoch: 32 [012/014], Loss: 0.631782\n",
      "Train Epoch: 32 [014/014], Loss: 0.625995\n",
      "Train Epoch: 33 [002/014], Loss: 0.633917\n",
      "Train Epoch: 33 [004/014], Loss: 0.638709\n",
      "Train Epoch: 33 [006/014], Loss: 0.621687\n",
      "Train Epoch: 33 [008/014], Loss: 0.629313\n",
      "Train Epoch: 33 [010/014], Loss: 0.649509\n",
      "Train Epoch: 33 [012/014], Loss: 0.629397\n",
      "Train Epoch: 33 [014/014], Loss: 0.625931\n",
      "Train Epoch: 34 [002/014], Loss: 0.626114\n",
      "Train Epoch: 34 [004/014], Loss: 0.633989\n",
      "Train Epoch: 34 [006/014], Loss: 0.630610\n",
      "Train Epoch: 34 [008/014], Loss: 0.625031\n",
      "Train Epoch: 34 [010/014], Loss: 0.637979\n",
      "Train Epoch: 34 [012/014], Loss: 0.627199\n",
      "Train Epoch: 34 [014/014], Loss: 0.629517\n",
      "Train Epoch: 35 [002/014], Loss: 0.632530\n",
      "Train Epoch: 35 [004/014], Loss: 0.636707\n",
      "Train Epoch: 35 [006/014], Loss: 0.639252\n",
      "Train Epoch: 35 [008/014], Loss: 0.621748\n",
      "Train Epoch: 35 [010/014], Loss: 0.627748\n",
      "Train Epoch: 35 [012/014], Loss: 0.629185\n",
      "Train Epoch: 35 [014/014], Loss: 0.646833\n",
      "Train Epoch: 36 [002/014], Loss: 0.620424\n",
      "Train Epoch: 36 [004/014], Loss: 0.639354\n",
      "Train Epoch: 36 [006/014], Loss: 0.623190\n",
      "Train Epoch: 36 [008/014], Loss: 0.637349\n",
      "Train Epoch: 36 [010/014], Loss: 0.635256\n",
      "Train Epoch: 36 [012/014], Loss: 0.637949\n",
      "Train Epoch: 36 [014/014], Loss: 0.633063\n",
      "Train Epoch: 37 [002/014], Loss: 0.630516\n",
      "Train Epoch: 37 [004/014], Loss: 0.634485\n",
      "Train Epoch: 37 [006/014], Loss: 0.629304\n",
      "Train Epoch: 37 [008/014], Loss: 0.639938\n",
      "Train Epoch: 37 [010/014], Loss: 0.623118\n",
      "Train Epoch: 37 [012/014], Loss: 0.634647\n",
      "Train Epoch: 37 [014/014], Loss: 0.633054\n",
      "Train Epoch: 38 [002/014], Loss: 0.625281\n",
      "Train Epoch: 38 [004/014], Loss: 0.631059\n",
      "Train Epoch: 38 [006/014], Loss: 0.631925\n",
      "Train Epoch: 38 [008/014], Loss: 0.632768\n",
      "Train Epoch: 38 [010/014], Loss: 0.619665\n",
      "Train Epoch: 38 [012/014], Loss: 0.637239\n",
      "Train Epoch: 38 [014/014], Loss: 0.613919\n",
      "Train Epoch: 39 [002/014], Loss: 0.631310\n",
      "Train Epoch: 39 [004/014], Loss: 0.628567\n",
      "Train Epoch: 39 [006/014], Loss: 0.628564\n",
      "Train Epoch: 39 [008/014], Loss: 0.628488\n",
      "Train Epoch: 39 [010/014], Loss: 0.629140\n",
      "Train Epoch: 39 [012/014], Loss: 0.638221\n",
      "Train Epoch: 39 [014/014], Loss: 0.617407\n",
      "Train Epoch: 40 [002/014], Loss: 0.635983\n",
      "Train Epoch: 40 [004/014], Loss: 0.643436\n",
      "Train Epoch: 40 [006/014], Loss: 0.628611\n",
      "Train Epoch: 40 [008/014], Loss: 0.618301\n",
      "Train Epoch: 40 [010/014], Loss: 0.633905\n",
      "Train Epoch: 40 [012/014], Loss: 0.636024\n",
      "Train Epoch: 40 [014/014], Loss: 0.632899\n",
      "Train Epoch: 41 [002/014], Loss: 0.639956\n",
      "Train Epoch: 41 [004/014], Loss: 0.625769\n",
      "Train Epoch: 41 [006/014], Loss: 0.626613\n",
      "Train Epoch: 41 [008/014], Loss: 0.634524\n",
      "Train Epoch: 41 [010/014], Loss: 0.637492\n",
      "Train Epoch: 41 [012/014], Loss: 0.649532\n",
      "Train Epoch: 41 [014/014], Loss: 0.631181\n",
      "Train Epoch: 42 [002/014], Loss: 0.626379\n",
      "Train Epoch: 42 [004/014], Loss: 0.640723\n",
      "Train Epoch: 42 [006/014], Loss: 0.637478\n",
      "Train Epoch: 42 [008/014], Loss: 0.631952\n",
      "Train Epoch: 42 [010/014], Loss: 0.627720\n",
      "Train Epoch: 42 [012/014], Loss: 0.634673\n",
      "Train Epoch: 42 [014/014], Loss: 0.641718\n",
      "Train Epoch: 43 [002/014], Loss: 0.629836\n",
      "Train Epoch: 43 [004/014], Loss: 0.632923\n",
      "Train Epoch: 43 [006/014], Loss: 0.632450\n",
      "Train Epoch: 43 [008/014], Loss: 0.635421\n",
      "Train Epoch: 43 [010/014], Loss: 0.625139\n",
      "Train Epoch: 43 [012/014], Loss: 0.628362\n",
      "Train Epoch: 43 [014/014], Loss: 0.626108\n",
      "Train Epoch: 44 [002/014], Loss: 0.626779\n",
      "Train Epoch: 44 [004/014], Loss: 0.635221\n",
      "Train Epoch: 44 [006/014], Loss: 0.639933\n",
      "Train Epoch: 44 [008/014], Loss: 0.626608\n",
      "Train Epoch: 44 [010/014], Loss: 0.636910\n",
      "Train Epoch: 44 [012/014], Loss: 0.633373\n",
      "Train Epoch: 44 [014/014], Loss: 0.651960\n",
      "Train Epoch: 45 [002/014], Loss: 0.624341\n",
      "Train Epoch: 45 [004/014], Loss: 0.636573\n",
      "Train Epoch: 45 [006/014], Loss: 0.636643\n",
      "Train Epoch: 45 [008/014], Loss: 0.635140\n",
      "Train Epoch: 45 [010/014], Loss: 0.629278\n",
      "Train Epoch: 45 [012/014], Loss: 0.628528\n",
      "Train Epoch: 45 [014/014], Loss: 0.619289\n",
      "Train Epoch: 46 [002/014], Loss: 0.631963\n",
      "Train Epoch: 46 [004/014], Loss: 0.637392\n",
      "Train Epoch: 46 [006/014], Loss: 0.630672\n",
      "Train Epoch: 46 [008/014], Loss: 0.637218\n",
      "Train Epoch: 46 [010/014], Loss: 0.640062\n",
      "Train Epoch: 46 [012/014], Loss: 0.630046\n",
      "Train Epoch: 46 [014/014], Loss: 0.643648\n",
      "Train Epoch: 47 [002/014], Loss: 0.634557\n",
      "Train Epoch: 47 [004/014], Loss: 0.626489\n",
      "Train Epoch: 47 [006/014], Loss: 0.623914\n",
      "Train Epoch: 47 [008/014], Loss: 0.628714\n",
      "Train Epoch: 47 [010/014], Loss: 0.637825\n",
      "Train Epoch: 47 [012/014], Loss: 0.635253\n",
      "Train Epoch: 47 [014/014], Loss: 0.639779\n",
      "Train Epoch: 48 [002/014], Loss: 0.628533\n",
      "Train Epoch: 48 [004/014], Loss: 0.634061\n",
      "Train Epoch: 48 [006/014], Loss: 0.632015\n",
      "Train Epoch: 48 [008/014], Loss: 0.635132\n",
      "Train Epoch: 48 [010/014], Loss: 0.633853\n",
      "Train Epoch: 48 [012/014], Loss: 0.634593\n",
      "Train Epoch: 48 [014/014], Loss: 0.646779\n",
      "Train Epoch: 49 [002/014], Loss: 0.638833\n",
      "Train Epoch: 49 [004/014], Loss: 0.631222\n",
      "Train Epoch: 49 [006/014], Loss: 0.644714\n",
      "Train Epoch: 49 [008/014], Loss: 0.637625\n",
      "Train Epoch: 49 [010/014], Loss: 0.633793\n",
      "Train Epoch: 49 [012/014], Loss: 0.637378\n",
      "Train Epoch: 49 [014/014], Loss: 0.638146\n",
      "Train Epoch: 50 [002/014], Loss: 0.629184\n",
      "Train Epoch: 50 [004/014], Loss: 0.635900\n",
      "Train Epoch: 50 [006/014], Loss: 0.633863\n",
      "Train Epoch: 50 [008/014], Loss: 0.633990\n",
      "Train Epoch: 50 [010/014], Loss: 0.631434\n",
      "Train Epoch: 50 [012/014], Loss: 0.632465\n",
      "Train Epoch: 50 [014/014], Loss: 0.632941\n",
      "Train Epoch: 51 [002/014], Loss: 0.633274\n",
      "Train Epoch: 51 [004/014], Loss: 0.629863\n",
      "Train Epoch: 51 [006/014], Loss: 0.629258\n",
      "Train Epoch: 51 [008/014], Loss: 0.633396\n",
      "Train Epoch: 51 [010/014], Loss: 0.630608\n",
      "Train Epoch: 51 [012/014], Loss: 0.637152\n",
      "Train Epoch: 51 [014/014], Loss: 0.629565\n",
      "Train Epoch: 52 [002/014], Loss: 0.634521\n",
      "Train Epoch: 52 [004/014], Loss: 0.629181\n",
      "Train Epoch: 52 [006/014], Loss: 0.629819\n",
      "Train Epoch: 52 [008/014], Loss: 0.630670\n",
      "Train Epoch: 52 [010/014], Loss: 0.642031\n",
      "Train Epoch: 52 [012/014], Loss: 0.632570\n",
      "Train Epoch: 52 [014/014], Loss: 0.626334\n",
      "Train Epoch: 53 [002/014], Loss: 0.633066\n",
      "Train Epoch: 53 [004/014], Loss: 0.638722\n",
      "Train Epoch: 53 [006/014], Loss: 0.624531\n",
      "Train Epoch: 53 [008/014], Loss: 0.639438\n",
      "Train Epoch: 53 [010/014], Loss: 0.636751\n",
      "Train Epoch: 53 [012/014], Loss: 0.629762\n",
      "Train Epoch: 53 [014/014], Loss: 0.650243\n",
      "Train Epoch: 54 [002/014], Loss: 0.641590\n",
      "Train Epoch: 54 [004/014], Loss: 0.632805\n",
      "Train Epoch: 54 [006/014], Loss: 0.626517\n",
      "Train Epoch: 54 [008/014], Loss: 0.637261\n",
      "Train Epoch: 54 [010/014], Loss: 0.633182\n",
      "Train Epoch: 54 [012/014], Loss: 0.628493\n",
      "Train Epoch: 54 [014/014], Loss: 0.617371\n",
      "Train Epoch: 55 [002/014], Loss: 0.622304\n",
      "Train Epoch: 55 [004/014], Loss: 0.643261\n",
      "Train Epoch: 55 [006/014], Loss: 0.632228\n",
      "Train Epoch: 55 [008/014], Loss: 0.628598\n",
      "Train Epoch: 55 [010/014], Loss: 0.623145\n",
      "Train Epoch: 55 [012/014], Loss: 0.635974\n",
      "Train Epoch: 55 [014/014], Loss: 0.624574\n",
      "Train Epoch: 56 [002/014], Loss: 0.635921\n",
      "Train Epoch: 56 [004/014], Loss: 0.625924\n",
      "Train Epoch: 56 [006/014], Loss: 0.633200\n",
      "Train Epoch: 56 [008/014], Loss: 0.631820\n",
      "Train Epoch: 56 [010/014], Loss: 0.629737\n",
      "Train Epoch: 56 [012/014], Loss: 0.623771\n",
      "Train Epoch: 56 [014/014], Loss: 0.643421\n",
      "Train Epoch: 57 [002/014], Loss: 0.638662\n",
      "Train Epoch: 57 [004/014], Loss: 0.640116\n",
      "Train Epoch: 57 [006/014], Loss: 0.622403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [008/014], Loss: 0.622062\n",
      "Train Epoch: 57 [010/014], Loss: 0.637211\n",
      "Train Epoch: 57 [012/014], Loss: 0.633086\n",
      "Train Epoch: 57 [014/014], Loss: 0.638193\n",
      "Train Epoch: 58 [002/014], Loss: 0.632646\n",
      "Train Epoch: 58 [004/014], Loss: 0.624598\n",
      "Train Epoch: 58 [006/014], Loss: 0.637356\n",
      "Train Epoch: 58 [008/014], Loss: 0.631152\n",
      "Train Epoch: 58 [010/014], Loss: 0.639878\n",
      "Train Epoch: 58 [012/014], Loss: 0.642697\n",
      "Train Epoch: 58 [014/014], Loss: 0.627925\n",
      "Train Epoch: 59 [002/014], Loss: 0.631192\n",
      "Train Epoch: 59 [004/014], Loss: 0.635158\n",
      "Train Epoch: 59 [006/014], Loss: 0.638561\n",
      "Train Epoch: 59 [008/014], Loss: 0.616585\n",
      "Train Epoch: 59 [010/014], Loss: 0.632052\n",
      "Train Epoch: 59 [012/014], Loss: 0.632480\n",
      "Train Epoch: 59 [014/014], Loss: 0.622627\n",
      "Train Epoch: 60 [002/014], Loss: 0.620523\n",
      "Train Epoch: 60 [004/014], Loss: 0.639923\n",
      "Train Epoch: 60 [006/014], Loss: 0.633224\n",
      "Train Epoch: 60 [008/014], Loss: 0.634460\n",
      "Train Epoch: 60 [010/014], Loss: 0.634659\n",
      "Train Epoch: 60 [012/014], Loss: 0.637998\n",
      "Train Epoch: 60 [014/014], Loss: 0.622621\n",
      "Train Epoch: 61 [002/014], Loss: 0.628037\n",
      "Train Epoch: 61 [004/014], Loss: 0.642649\n",
      "Train Epoch: 61 [006/014], Loss: 0.635889\n",
      "Train Epoch: 61 [008/014], Loss: 0.624989\n",
      "Train Epoch: 61 [010/014], Loss: 0.642832\n",
      "Train Epoch: 61 [012/014], Loss: 0.622432\n",
      "Train Epoch: 61 [014/014], Loss: 0.626114\n",
      "Train Epoch: 62 [002/014], Loss: 0.626525\n",
      "Train Epoch: 62 [004/014], Loss: 0.621087\n",
      "Train Epoch: 62 [006/014], Loss: 0.631377\n",
      "Train Epoch: 62 [008/014], Loss: 0.639992\n",
      "Train Epoch: 62 [010/014], Loss: 0.636019\n",
      "Train Epoch: 62 [012/014], Loss: 0.643512\n",
      "Train Epoch: 62 [014/014], Loss: 0.641523\n",
      "Train Epoch: 63 [002/014], Loss: 0.633765\n",
      "Train Epoch: 63 [004/014], Loss: 0.633180\n",
      "Train Epoch: 63 [006/014], Loss: 0.638862\n",
      "Train Epoch: 63 [008/014], Loss: 0.619042\n",
      "Train Epoch: 63 [010/014], Loss: 0.643293\n",
      "Train Epoch: 63 [012/014], Loss: 0.628605\n",
      "Train Epoch: 63 [014/014], Loss: 0.634876\n",
      "Train Epoch: 64 [002/014], Loss: 0.635994\n",
      "Train Epoch: 64 [004/014], Loss: 0.628624\n",
      "Train Epoch: 64 [006/014], Loss: 0.622218\n",
      "Train Epoch: 64 [008/014], Loss: 0.627975\n",
      "Train Epoch: 64 [010/014], Loss: 0.638609\n",
      "Train Epoch: 64 [012/014], Loss: 0.629179\n",
      "Train Epoch: 64 [014/014], Loss: 0.631222\n",
      "Train Epoch: 65 [002/014], Loss: 0.624450\n",
      "Train Epoch: 65 [004/014], Loss: 0.621882\n",
      "Train Epoch: 65 [006/014], Loss: 0.634494\n",
      "Train Epoch: 65 [008/014], Loss: 0.629328\n",
      "Train Epoch: 65 [010/014], Loss: 0.638807\n",
      "Train Epoch: 65 [012/014], Loss: 0.627787\n",
      "Train Epoch: 65 [014/014], Loss: 0.643431\n",
      "Train Epoch: 66 [002/014], Loss: 0.624606\n",
      "Train Epoch: 66 [004/014], Loss: 0.638786\n",
      "Train Epoch: 66 [006/014], Loss: 0.635443\n",
      "Train Epoch: 66 [008/014], Loss: 0.634642\n",
      "Train Epoch: 66 [010/014], Loss: 0.632138\n",
      "Train Epoch: 66 [012/014], Loss: 0.642024\n",
      "Train Epoch: 66 [014/014], Loss: 0.641587\n",
      "Train Epoch: 67 [002/014], Loss: 0.627129\n",
      "Train Epoch: 67 [004/014], Loss: 0.628596\n",
      "Train Epoch: 67 [006/014], Loss: 0.623959\n",
      "Train Epoch: 67 [008/014], Loss: 0.632647\n",
      "Train Epoch: 67 [010/014], Loss: 0.629849\n",
      "Train Epoch: 67 [012/014], Loss: 0.626553\n",
      "Train Epoch: 67 [014/014], Loss: 0.632980\n",
      "Train Epoch: 68 [002/014], Loss: 0.628559\n",
      "Train Epoch: 68 [004/014], Loss: 0.635219\n",
      "Train Epoch: 68 [006/014], Loss: 0.622570\n",
      "Train Epoch: 68 [008/014], Loss: 0.631851\n",
      "Train Epoch: 68 [010/014], Loss: 0.636614\n",
      "Train Epoch: 68 [012/014], Loss: 0.626339\n",
      "Train Epoch: 68 [014/014], Loss: 0.608840\n",
      "Train Epoch: 69 [002/014], Loss: 0.641914\n",
      "Train Epoch: 69 [004/014], Loss: 0.631223\n",
      "Train Epoch: 69 [006/014], Loss: 0.631774\n",
      "Train Epoch: 69 [008/014], Loss: 0.633874\n",
      "Train Epoch: 69 [010/014], Loss: 0.645949\n",
      "Train Epoch: 69 [012/014], Loss: 0.636509\n",
      "Train Epoch: 69 [014/014], Loss: 0.612193\n",
      "Train Epoch: 70 [002/014], Loss: 0.633217\n",
      "Train Epoch: 70 [004/014], Loss: 0.630695\n",
      "Train Epoch: 70 [006/014], Loss: 0.634542\n",
      "Train Epoch: 70 [008/014], Loss: 0.630622\n",
      "Train Epoch: 70 [010/014], Loss: 0.637412\n",
      "Train Epoch: 70 [012/014], Loss: 0.638078\n",
      "Train Epoch: 70 [014/014], Loss: 0.648464\n",
      "Train Epoch: 71 [002/014], Loss: 0.633151\n",
      "Train Epoch: 71 [004/014], Loss: 0.620547\n",
      "Train Epoch: 71 [006/014], Loss: 0.623849\n",
      "Train Epoch: 71 [008/014], Loss: 0.629822\n",
      "Train Epoch: 71 [010/014], Loss: 0.639944\n",
      "Train Epoch: 71 [012/014], Loss: 0.632621\n",
      "Train Epoch: 71 [014/014], Loss: 0.636733\n",
      "Train Epoch: 72 [002/014], Loss: 0.630027\n",
      "Train Epoch: 72 [004/014], Loss: 0.623873\n",
      "Train Epoch: 72 [006/014], Loss: 0.618413\n",
      "Train Epoch: 72 [008/014], Loss: 0.637182\n",
      "Train Epoch: 72 [010/014], Loss: 0.641562\n",
      "Train Epoch: 72 [012/014], Loss: 0.626619\n",
      "Train Epoch: 72 [014/014], Loss: 0.650347\n",
      "Train Epoch: 73 [002/014], Loss: 0.632290\n",
      "Train Epoch: 73 [004/014], Loss: 0.631882\n",
      "Train Epoch: 73 [006/014], Loss: 0.627878\n",
      "Train Epoch: 73 [008/014], Loss: 0.640823\n",
      "Train Epoch: 73 [010/014], Loss: 0.626413\n",
      "Train Epoch: 73 [012/014], Loss: 0.638130\n",
      "Train Epoch: 73 [014/014], Loss: 0.633358\n",
      "Train Epoch: 74 [002/014], Loss: 0.627818\n",
      "Train Epoch: 74 [004/014], Loss: 0.632656\n",
      "Train Epoch: 74 [006/014], Loss: 0.614199\n",
      "Train Epoch: 74 [008/014], Loss: 0.624516\n",
      "Train Epoch: 74 [010/014], Loss: 0.634759\n",
      "Train Epoch: 74 [012/014], Loss: 0.633180\n",
      "Train Epoch: 74 [014/014], Loss: 0.634646\n",
      "Train Epoch: 75 [002/014], Loss: 0.637922\n",
      "Train Epoch: 75 [004/014], Loss: 0.629472\n",
      "Train Epoch: 75 [006/014], Loss: 0.640785\n",
      "Train Epoch: 75 [008/014], Loss: 0.627820\n",
      "Train Epoch: 75 [010/014], Loss: 0.636747\n",
      "Train Epoch: 75 [012/014], Loss: 0.626462\n",
      "Train Epoch: 75 [014/014], Loss: 0.636516\n",
      "Train Epoch: 76 [002/014], Loss: 0.636737\n",
      "Train Epoch: 76 [004/014], Loss: 0.635329\n",
      "Train Epoch: 76 [006/014], Loss: 0.633000\n",
      "Train Epoch: 76 [008/014], Loss: 0.628365\n",
      "Train Epoch: 76 [010/014], Loss: 0.636766\n",
      "Train Epoch: 76 [012/014], Loss: 0.629198\n",
      "Train Epoch: 76 [014/014], Loss: 0.622680\n",
      "Train Epoch: 77 [002/014], Loss: 0.627109\n",
      "Train Epoch: 77 [004/014], Loss: 0.630402\n",
      "Train Epoch: 77 [006/014], Loss: 0.628468\n",
      "Train Epoch: 77 [008/014], Loss: 0.636706\n",
      "Train Epoch: 77 [010/014], Loss: 0.639906\n",
      "Train Epoch: 77 [012/014], Loss: 0.626446\n",
      "Train Epoch: 77 [014/014], Loss: 0.619229\n",
      "Train Epoch: 78 [002/014], Loss: 0.639380\n",
      "Train Epoch: 78 [004/014], Loss: 0.627865\n",
      "Train Epoch: 78 [006/014], Loss: 0.633972\n",
      "Train Epoch: 78 [008/014], Loss: 0.638545\n",
      "Train Epoch: 78 [010/014], Loss: 0.628431\n",
      "Train Epoch: 78 [012/014], Loss: 0.626394\n",
      "Train Epoch: 78 [014/014], Loss: 0.638132\n",
      "Train Epoch: 79 [002/014], Loss: 0.635901\n",
      "Train Epoch: 79 [004/014], Loss: 0.626472\n",
      "Train Epoch: 79 [006/014], Loss: 0.636465\n",
      "Train Epoch: 79 [008/014], Loss: 0.635803\n",
      "Train Epoch: 79 [010/014], Loss: 0.634735\n",
      "Train Epoch: 79 [012/014], Loss: 0.623673\n",
      "Train Epoch: 79 [014/014], Loss: 0.634713\n",
      "Train Epoch: 80 [002/014], Loss: 0.629070\n",
      "Train Epoch: 80 [004/014], Loss: 0.627994\n",
      "Train Epoch: 80 [006/014], Loss: 0.641536\n",
      "Train Epoch: 80 [008/014], Loss: 0.633317\n",
      "Train Epoch: 80 [010/014], Loss: 0.637922\n",
      "Train Epoch: 80 [012/014], Loss: 0.631292\n",
      "Train Epoch: 80 [014/014], Loss: 0.627838\n",
      "Train Epoch: 81 [002/014], Loss: 0.619114\n",
      "Train Epoch: 81 [004/014], Loss: 0.631196\n",
      "Train Epoch: 81 [006/014], Loss: 0.642914\n",
      "Train Epoch: 81 [008/014], Loss: 0.630488\n",
      "Train Epoch: 81 [010/014], Loss: 0.615055\n",
      "Train Epoch: 81 [012/014], Loss: 0.641361\n",
      "Train Epoch: 81 [014/014], Loss: 0.631104\n",
      "Train Epoch: 82 [002/014], Loss: 0.635320\n",
      "Train Epoch: 82 [004/014], Loss: 0.637287\n",
      "Train Epoch: 82 [006/014], Loss: 0.636561\n",
      "Train Epoch: 82 [008/014], Loss: 0.621778\n",
      "Train Epoch: 82 [010/014], Loss: 0.641237\n",
      "Train Epoch: 82 [012/014], Loss: 0.617485\n",
      "Train Epoch: 82 [014/014], Loss: 0.620861\n",
      "Train Epoch: 83 [002/014], Loss: 0.638713\n",
      "Train Epoch: 83 [004/014], Loss: 0.640652\n",
      "Train Epoch: 83 [006/014], Loss: 0.632612\n",
      "Train Epoch: 83 [008/014], Loss: 0.625198\n",
      "Train Epoch: 83 [010/014], Loss: 0.633802\n",
      "Train Epoch: 83 [012/014], Loss: 0.631252\n",
      "Train Epoch: 83 [014/014], Loss: 0.636480\n",
      "Train Epoch: 84 [002/014], Loss: 0.631007\n",
      "Train Epoch: 84 [004/014], Loss: 0.632599\n",
      "Train Epoch: 84 [006/014], Loss: 0.633214\n",
      "Train Epoch: 84 [008/014], Loss: 0.635776\n",
      "Train Epoch: 84 [010/014], Loss: 0.633852\n",
      "Train Epoch: 84 [012/014], Loss: 0.628577\n",
      "Train Epoch: 84 [014/014], Loss: 0.638130\n",
      "Train Epoch: 85 [002/014], Loss: 0.629273\n",
      "Train Epoch: 85 [004/014], Loss: 0.623215\n",
      "Train Epoch: 85 [006/014], Loss: 0.626612\n",
      "Train Epoch: 85 [008/014], Loss: 0.631411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 85 [010/014], Loss: 0.622553\n",
      "Train Epoch: 85 [012/014], Loss: 0.634601\n",
      "Train Epoch: 85 [014/014], Loss: 0.631189\n",
      "Train Epoch: 86 [002/014], Loss: 0.634040\n",
      "Train Epoch: 86 [004/014], Loss: 0.640888\n",
      "Train Epoch: 86 [006/014], Loss: 0.631850\n",
      "Train Epoch: 86 [008/014], Loss: 0.638060\n",
      "Train Epoch: 86 [010/014], Loss: 0.619138\n",
      "Train Epoch: 86 [012/014], Loss: 0.629097\n",
      "Train Epoch: 86 [014/014], Loss: 0.627756\n",
      "Train Epoch: 87 [002/014], Loss: 0.641956\n",
      "Train Epoch: 87 [004/014], Loss: 0.636731\n",
      "Train Epoch: 87 [006/014], Loss: 0.621635\n",
      "Train Epoch: 87 [008/014], Loss: 0.636807\n",
      "Train Epoch: 87 [010/014], Loss: 0.625470\n",
      "Train Epoch: 87 [012/014], Loss: 0.636612\n",
      "Train Epoch: 87 [014/014], Loss: 0.610670\n",
      "Train Epoch: 88 [002/014], Loss: 0.637948\n",
      "Train Epoch: 88 [004/014], Loss: 0.638689\n",
      "Train Epoch: 88 [006/014], Loss: 0.621212\n",
      "Train Epoch: 88 [008/014], Loss: 0.644140\n",
      "Train Epoch: 88 [010/014], Loss: 0.631334\n",
      "Train Epoch: 88 [012/014], Loss: 0.626480\n",
      "Train Epoch: 88 [014/014], Loss: 0.643277\n",
      "Train Epoch: 89 [002/014], Loss: 0.622601\n",
      "Train Epoch: 89 [004/014], Loss: 0.635357\n",
      "Train Epoch: 89 [006/014], Loss: 0.636216\n",
      "Train Epoch: 89 [008/014], Loss: 0.636499\n",
      "Train Epoch: 89 [010/014], Loss: 0.627102\n",
      "Train Epoch: 89 [012/014], Loss: 0.636637\n",
      "Train Epoch: 89 [014/014], Loss: 0.641783\n",
      "Train Epoch: 90 [002/014], Loss: 0.636077\n",
      "Train Epoch: 90 [004/014], Loss: 0.640059\n",
      "Train Epoch: 90 [006/014], Loss: 0.627589\n",
      "Train Epoch: 90 [008/014], Loss: 0.625672\n",
      "Train Epoch: 90 [010/014], Loss: 0.638849\n",
      "Train Epoch: 90 [012/014], Loss: 0.641447\n",
      "Train Epoch: 90 [014/014], Loss: 0.622694\n",
      "Train Epoch: 91 [002/014], Loss: 0.636470\n",
      "Train Epoch: 91 [004/014], Loss: 0.623823\n",
      "Train Epoch: 91 [006/014], Loss: 0.633880\n",
      "Train Epoch: 91 [008/014], Loss: 0.630487\n",
      "Train Epoch: 91 [010/014], Loss: 0.621016\n",
      "Train Epoch: 91 [012/014], Loss: 0.637282\n",
      "Train Epoch: 91 [014/014], Loss: 0.629576\n",
      "Train Epoch: 92 [002/014], Loss: 0.650237\n",
      "Train Epoch: 92 [004/014], Loss: 0.623715\n",
      "Train Epoch: 92 [006/014], Loss: 0.624150\n",
      "Train Epoch: 92 [008/014], Loss: 0.629236\n",
      "Train Epoch: 92 [010/014], Loss: 0.635464\n",
      "Train Epoch: 92 [012/014], Loss: 0.633238\n",
      "Train Epoch: 92 [014/014], Loss: 0.643329\n",
      "Train Epoch: 93 [002/014], Loss: 0.621738\n",
      "Train Epoch: 93 [004/014], Loss: 0.632555\n",
      "Train Epoch: 93 [006/014], Loss: 0.640005\n",
      "Train Epoch: 93 [008/014], Loss: 0.629092\n",
      "Train Epoch: 93 [010/014], Loss: 0.632563\n",
      "Train Epoch: 93 [012/014], Loss: 0.636703\n",
      "Train Epoch: 93 [014/014], Loss: 0.640115\n",
      "Train Epoch: 94 [002/014], Loss: 0.624405\n",
      "Train Epoch: 94 [004/014], Loss: 0.629766\n",
      "Train Epoch: 94 [006/014], Loss: 0.634570\n",
      "Train Epoch: 94 [008/014], Loss: 0.634677\n",
      "Train Epoch: 94 [010/014], Loss: 0.646822\n",
      "Train Epoch: 94 [012/014], Loss: 0.625054\n",
      "Train Epoch: 94 [014/014], Loss: 0.627774\n",
      "Train Epoch: 95 [002/014], Loss: 0.628540\n",
      "Train Epoch: 95 [004/014], Loss: 0.633320\n",
      "Train Epoch: 95 [006/014], Loss: 0.635363\n",
      "Train Epoch: 95 [008/014], Loss: 0.627776\n",
      "Train Epoch: 95 [010/014], Loss: 0.629346\n",
      "Train Epoch: 95 [012/014], Loss: 0.626781\n",
      "Train Epoch: 95 [014/014], Loss: 0.648529\n",
      "Train Epoch: 96 [002/014], Loss: 0.640580\n",
      "Train Epoch: 96 [004/014], Loss: 0.622305\n",
      "Train Epoch: 96 [006/014], Loss: 0.638357\n",
      "Train Epoch: 96 [008/014], Loss: 0.635408\n",
      "Train Epoch: 96 [010/014], Loss: 0.642660\n",
      "Train Epoch: 96 [012/014], Loss: 0.627035\n",
      "Train Epoch: 96 [014/014], Loss: 0.646768\n",
      "Train Epoch: 97 [002/014], Loss: 0.633939\n",
      "Train Epoch: 97 [004/014], Loss: 0.628568\n",
      "Train Epoch: 97 [006/014], Loss: 0.648825\n",
      "Train Epoch: 97 [008/014], Loss: 0.635110\n",
      "Train Epoch: 97 [010/014], Loss: 0.631891\n",
      "Train Epoch: 97 [012/014], Loss: 0.634586\n",
      "Train Epoch: 97 [014/014], Loss: 0.615650\n",
      "Train Epoch: 98 [002/014], Loss: 0.635204\n",
      "Train Epoch: 98 [004/014], Loss: 0.635237\n",
      "Train Epoch: 98 [006/014], Loss: 0.636590\n",
      "Train Epoch: 98 [008/014], Loss: 0.632796\n",
      "Train Epoch: 98 [010/014], Loss: 0.633805\n",
      "Train Epoch: 98 [012/014], Loss: 0.621324\n",
      "Train Epoch: 98 [014/014], Loss: 0.627884\n",
      "Train Epoch: 99 [002/014], Loss: 0.628411\n",
      "Train Epoch: 99 [004/014], Loss: 0.637174\n",
      "Train Epoch: 99 [006/014], Loss: 0.638462\n",
      "Train Epoch: 99 [008/014], Loss: 0.633882\n",
      "Train Epoch: 99 [010/014], Loss: 0.620355\n",
      "Train Epoch: 99 [012/014], Loss: 0.639376\n",
      "Train Epoch: 99 [014/014], Loss: 0.634564\n",
      "Train Epoch: 100 [002/014], Loss: 0.633126\n",
      "Train Epoch: 100 [004/014], Loss: 0.628578\n",
      "Train Epoch: 100 [006/014], Loss: 0.626511\n",
      "Train Epoch: 100 [008/014], Loss: 0.633340\n",
      "Train Epoch: 100 [010/014], Loss: 0.640637\n",
      "Train Epoch: 100 [012/014], Loss: 0.638139\n",
      "Train Epoch: 100 [014/014], Loss: 0.641520\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "        scheduler.step()\n",
    "        train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "test_dataset = utils.TensorDataset(torch.FloatTensor(X_test),\n",
    "                                   torch.FloatTensor(y_test))\n",
    "test_dataloader = utils.DataLoader(test_dataset,batch_size=24)\n",
    "\n",
    "y_pred_lst = []\n",
    "y_truth_lst = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (X, y) in enumerate(test_dataloader):\n",
    "        \n",
    "        output = likelihood(model(X.cuda()))\n",
    "        \n",
    "        y_pred = output.mean.ge(0.5).float().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        y_pred_lst.append(y_pred)\n",
    "        y_truth_lst.append(y.numpy())\n",
    "    truth = np.concatenate(y_truth_lst)\n",
    "    pred =  np.concatenate(y_pred_lst)\n",
    "    print(truth)\n",
    "    print(pred)\n",
    "    auc = roc_auc_score(truth,pred)\n",
    "                   \n",
    "print(auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amlenv]",
   "language": "python",
   "name": "conda-env-amlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
